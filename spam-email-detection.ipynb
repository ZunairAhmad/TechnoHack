{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb32dbab",
   "metadata": {
    "id": "yl7xoxcBfjdz",
    "papermill": {
     "duration": 0.005657,
     "end_time": "2023-10-15T10:20:57.268220",
     "exception": false,
     "start_time": "2023-10-15T10:20:57.262563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "#Explanation:\n",
    "\n",
    "We import the pandas library using import pandas as pd.\n",
    "\n",
    "We use pd.read_csv() to read the CSV file containing the dataset. The encoding='latin-1' argument is used to handle special characters.\n",
    "\n",
    "We select only the relevant columns ('v1' for labels, 'v2' for email content) using data[['v1', 'v2']]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5c4a93a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T10:20:57.283316Z",
     "iopub.status.busy": "2023-10-15T10:20:57.282530Z",
     "iopub.status.idle": "2023-10-15T10:20:57.728825Z",
     "shell.execute_reply": "2023-10-15T10:20:57.727422Z"
    },
    "id": "JtAeDqdNCOYm",
    "papermill": {
     "duration": 0.456808,
     "end_time": "2023-10-15T10:20:57.731513",
     "exception": false,
     "start_time": "2023-10-15T10:20:57.274705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('/kaggle/input/sms-spam-collection-dataset/spam.csv', encoding='latin-1')\n",
    "data = data[['v1', 'v2']]  # Selecting only the relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91229e6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T10:20:57.745720Z",
     "iopub.status.busy": "2023-10-15T10:20:57.745318Z",
     "iopub.status.idle": "2023-10-15T10:20:57.760370Z",
     "shell.execute_reply": "2023-10-15T10:20:57.759645Z"
    },
    "id": "vL2c3VWcDmtu",
    "outputId": "96824c1d-cf3c-457c-ff92-fb0fc6fb4df2",
    "papermill": {
     "duration": 0.025246,
     "end_time": "2023-10-15T10:20:57.762824",
     "exception": false,
     "start_time": "2023-10-15T10:20:57.737578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham              Will Ì_ b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data #printing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d475e71",
   "metadata": {
    "id": "uz1TyJN3H9F9",
    "papermill": {
     "duration": 0.006163,
     "end_time": "2023-10-15T10:20:57.775708",
     "exception": false,
     "start_time": "2023-10-15T10:20:57.769545",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2534391d",
   "metadata": {
    "id": "kQ8Q1vgLfqvx",
    "papermill": {
     "duration": 0.006149,
     "end_time": "2023-10-15T10:20:57.788587",
     "exception": false,
     "start_time": "2023-10-15T10:20:57.782438",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this step, we perform data preprocessing tasks, which include converting labels to binary values and removing duplicates from the dataset.\n",
    "\n",
    "#Explanation:\n",
    "\n",
    "We use data['v1'].apply(lambda x: 1 if x == 'spam' else 0) to convert the labels. 'ham' is mapped to 0, and 'spam' is mapped to 1 in the 'v1' column.\n",
    "\n",
    "We then remove duplicate rows from the dataset using data = data.drop_duplicates().\n",
    "\n",
    "The resulting DataFrame is displayed to show the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "828c0069",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T10:20:57.803860Z",
     "iopub.status.busy": "2023-10-15T10:20:57.802724Z",
     "iopub.status.idle": "2023-10-15T10:20:57.820080Z",
     "shell.execute_reply": "2023-10-15T10:20:57.819269Z"
    },
    "id": "4E88TDh7Dn6J",
    "papermill": {
     "duration": 0.027767,
     "end_time": "2023-10-15T10:20:57.822494",
     "exception": false,
     "start_time": "2023-10-15T10:20:57.794727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert 'ham' to 0 and 'spam' to 1 directly in the 'v1' column\n",
    "data['v1'] = data['v1'].apply(lambda x: 1 if x == 'spam' else 0)\n",
    "\n",
    "# removing duplicates\n",
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "709dc05a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T10:20:57.837608Z",
     "iopub.status.busy": "2023-10-15T10:20:57.837163Z",
     "iopub.status.idle": "2023-10-15T10:20:57.849924Z",
     "shell.execute_reply": "2023-10-15T10:20:57.848868Z"
    },
    "id": "xxcMJq0TQC6t",
    "outputId": "5e68a618-66a3-4516-b380-9a49ddd8f5bc",
    "papermill": {
     "duration": 0.023062,
     "end_time": "2023-10-15T10:20:57.852113",
     "exception": false,
     "start_time": "2023-10-15T10:20:57.829051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5169 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      v1                                                 v2\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro...\n",
       "...   ..                                                ...\n",
       "5567   1  This is the 2nd time we have tried 2 contact u...\n",
       "5568   0              Will Ì_ b going to esplanade fr home?\n",
       "5569   0  Pity, * was in mood for that. So...any other s...\n",
       "5570   0  The guy did some bitching but I acted like i'd...\n",
       "5571   0                         Rofl. Its true to its name\n",
       "\n",
       "[5169 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5edf1ea",
   "metadata": {
    "id": "ARvdY2iSgGVM",
    "papermill": {
     "duration": 0.006407,
     "end_time": "2023-10-15T10:20:57.865583",
     "exception": false,
     "start_time": "2023-10-15T10:20:57.859176",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To prevent the \"SettingWithCopyWarning\" that can occur when making changes to a slice of a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36074820",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T10:20:57.880310Z",
     "iopub.status.busy": "2023-10-15T10:20:57.879859Z",
     "iopub.status.idle": "2023-10-15T10:20:57.885298Z",
     "shell.execute_reply": "2023-10-15T10:20:57.884125Z"
    },
    "id": "lyMmh3bARmgz",
    "papermill": {
     "duration": 0.015574,
     "end_time": "2023-10-15T10:20:57.887680",
     "exception": false,
     "start_time": "2023-10-15T10:20:57.872106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # Disable the warning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e076457",
   "metadata": {
    "id": "G-95E4iJRAZc",
    "papermill": {
     "duration": 0.006548,
     "end_time": "2023-10-15T10:20:57.900986",
     "exception": false,
     "start_time": "2023-10-15T10:20:57.894438",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#Text Cleaning:\n",
    "\n",
    "Text cleaning involves removing any unnecessary characters, symbols, or noise from the text data. This might include punctuation, special characters, and numbers.\n",
    "\n",
    "#Explanation:\n",
    "\n",
    "We import the regular expression (re) module using import re.\n",
    "\n",
    "The function clean_text() takes a string text as input and uses a regular expression to remove all characters except alphabetic characters (letters).\n",
    "\n",
    "The cleaned text is then returned.\n",
    "\n",
    "We apply this function to the 'v2' column of the DataFrame using data['v2'].apply(lambda x: clean_text(x)). This cleans the text in each email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5e6e39d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T10:20:57.917968Z",
     "iopub.status.busy": "2023-10-15T10:20:57.916778Z",
     "iopub.status.idle": "2023-10-15T10:20:57.961169Z",
     "shell.execute_reply": "2023-10-15T10:20:57.960131Z"
    },
    "id": "fz6HutVZQ_yB",
    "papermill": {
     "duration": 0.055497,
     "end_time": "2023-10-15T10:20:57.963567",
     "exception": false,
     "start_time": "2023-10-15T10:20:57.908070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "data['v2'] = data['v2'].apply(lambda x: clean_text(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe3e6b4",
   "metadata": {
    "id": "TIA14UkdRurK",
    "papermill": {
     "duration": 0.006219,
     "end_time": "2023-10-15T10:20:57.976450",
     "exception": false,
     "start_time": "2023-10-15T10:20:57.970231",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#Lowercasing:\n",
    "\n",
    "Converting all text to lowercase ensures that the model doesn't treat \"Hello\" and \"hello\" as different words.\n",
    "\n",
    "#Explanation:\n",
    "\n",
    "We use the str.lower() method to convert all text in the 'v2' column to lowercase. This helps standardize the text data and ensure that the model is not case-sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1930aa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T10:20:57.991383Z",
     "iopub.status.busy": "2023-10-15T10:20:57.990748Z",
     "iopub.status.idle": "2023-10-15T10:20:57.998732Z",
     "shell.execute_reply": "2023-10-15T10:20:57.997481Z"
    },
    "id": "dy8u9x32RtBU",
    "papermill": {
     "duration": 0.018053,
     "end_time": "2023-10-15T10:20:58.000874",
     "exception": false,
     "start_time": "2023-10-15T10:20:57.982821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['v2'] = data['v2'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e667828",
   "metadata": {
    "id": "9Kc1mmwoPqPm",
    "papermill": {
     "duration": 0.006582,
     "end_time": "2023-10-15T10:20:58.014145",
     "exception": false,
     "start_time": "2023-10-15T10:20:58.007563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#Tokenization:\n",
    "\n",
    "Tokenization involves splitting the text into individual words or tokens. The NLTK library can be used for this.\n",
    "\n",
    "#Explanation:\n",
    "\n",
    "In this code cell, we use nltk.download('punkt') to download the necessary resources for tokenization from the Natural Language Toolkit (NLTK). This resource includes pre-trained models for tokenizing text into words or sentences. This step is essential for further text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "853f4fdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T10:20:58.029552Z",
     "iopub.status.busy": "2023-10-15T10:20:58.029078Z",
     "iopub.status.idle": "2023-10-15T10:21:00.389661Z",
     "shell.execute_reply": "2023-10-15T10:21:00.388353Z"
    },
    "id": "yczYXeHjSBhV",
    "outputId": "5af103ce-899f-411b-8b90-9e0435230659",
    "papermill": {
     "duration": 2.370854,
     "end_time": "2023-10-15T10:21:00.391970",
     "exception": false,
     "start_time": "2023-10-15T10:20:58.021116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f98139",
   "metadata": {
    "id": "ByDT5dV8gzQi",
    "papermill": {
     "duration": 0.006555,
     "end_time": "2023-10-15T10:21:00.405308",
     "exception": false,
     "start_time": "2023-10-15T10:21:00.398753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#Explanation:\n",
    "\n",
    "We import the word_tokenize function from the NLTK library.\n",
    "\n",
    "The word_tokenize function takes a string as input and returns a list of tokens (words).\n",
    "\n",
    "We apply this function to the 'v2' column of the DataFrame, converting each email's content into a list of tokens. This step is crucial for converting text data into a format suitable for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c57cd078",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T10:21:00.420524Z",
     "iopub.status.busy": "2023-10-15T10:21:00.420114Z",
     "iopub.status.idle": "2023-10-15T10:21:01.034954Z",
     "shell.execute_reply": "2023-10-15T10:21:01.033944Z"
    },
    "id": "-2FuT-MeR1_y",
    "papermill": {
     "duration": 0.625574,
     "end_time": "2023-10-15T10:21:01.037574",
     "exception": false,
     "start_time": "2023-10-15T10:21:00.412000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data['v2'] = data['v2'].apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f5eb6f",
   "metadata": {
    "id": "hyghFqqRSJdR",
    "papermill": {
     "duration": 0.006314,
     "end_time": "2023-10-15T10:21:01.051016",
     "exception": false,
     "start_time": "2023-10-15T10:21:01.044702",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#Stemming:\n",
    "\n",
    "Stemming reduces words to their base forms. This can help in reducing the dimensionality of the feature space.\n",
    "\n",
    "#Explanation:\n",
    "\n",
    "We import the PorterStemmer class from the NLTK library.\n",
    "\n",
    "We initialize an instance of the PorterStemmer as stemmer.\n",
    "\n",
    "We define a function stem_words(words) that takes a list of words and applies stemming to each word using the stemmer.stem() method.\n",
    "\n",
    "We apply this function to the 'v2' column of the DataFrame, effectively reducing words to their base forms through stemming. This step can help improve the model's performance by reducing the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d92576e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T10:21:01.066336Z",
     "iopub.status.busy": "2023-10-15T10:21:01.065353Z",
     "iopub.status.idle": "2023-10-15T10:21:02.221113Z",
     "shell.execute_reply": "2023-10-15T10:21:02.220131Z"
    },
    "id": "3nf29FaLSK5J",
    "papermill": {
     "duration": 1.166019,
     "end_time": "2023-10-15T10:21:02.223508",
     "exception": false,
     "start_time": "2023-10-15T10:21:01.057489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(words):\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "data['v2'] = data['v2'].apply(stem_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "028e04b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T10:21:02.238158Z",
     "iopub.status.busy": "2023-10-15T10:21:02.237816Z",
     "iopub.status.idle": "2023-10-15T10:21:02.252522Z",
     "shell.execute_reply": "2023-10-15T10:21:02.251219Z"
    },
    "id": "B-rmkk00SzRP",
    "outputId": "4d9e3835-f74e-4a95-9116-0f0cb8879ba7",
    "papermill": {
     "duration": 0.024585,
     "end_time": "2023-10-15T10:21:02.254542",
     "exception": false,
     "start_time": "2023-10-15T10:21:02.229957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[go, until, jurong, point, crazi, avail, onli,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[free, entri, in, a, wkli, comp, to, win, fa, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[u, dun, say, so, earli, hor, u, c, alreadi, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[nah, i, don, t, think, he, goe, to, usf, he, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>[thi, is, the, nd, time, we, have, tri, contac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>[will, b, go, to, esplanad, fr, home]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>[piti, wa, in, mood, for, that, so, ani, other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>[the, guy, did, some, bitch, but, i, act, like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>[rofl, it, true, to, it, name]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5169 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      v1                                                 v2\n",
       "0      0  [go, until, jurong, point, crazi, avail, onli,...\n",
       "1      0                       [ok, lar, joke, wif, u, oni]\n",
       "2      1  [free, entri, in, a, wkli, comp, to, win, fa, ...\n",
       "3      0  [u, dun, say, so, earli, hor, u, c, alreadi, t...\n",
       "4      0  [nah, i, don, t, think, he, goe, to, usf, he, ...\n",
       "...   ..                                                ...\n",
       "5567   1  [thi, is, the, nd, time, we, have, tri, contac...\n",
       "5568   0              [will, b, go, to, esplanad, fr, home]\n",
       "5569   0  [piti, wa, in, mood, for, that, so, ani, other...\n",
       "5570   0  [the, guy, did, some, bitch, but, i, act, like...\n",
       "5571   0                     [rofl, it, true, to, it, name]\n",
       "\n",
       "[5169 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2092441",
   "metadata": {
    "id": "DzboTMXJIDwT",
    "papermill": {
     "duration": 0.006581,
     "end_time": "2023-10-15T10:21:02.268429",
     "exception": false,
     "start_time": "2023-10-15T10:21:02.261848",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#Feature Extraction\n",
    "we convert the tokenized words back to text and apply Count Vectorization to transform the text data into numerical format.\n",
    "\n",
    "#Explanation:\n",
    "\n",
    "We import the CountVectorizer class from the scikit-learn library.\n",
    "\n",
    "We convert the tokenized words back to text using data['v2'].apply(lambda x: ' '.join(x)). This step is necessary for the Count Vectorizer to work correctly.\n",
    "\n",
    "We initialize the Count Vectorizer with a maximum of 5000 features using CountVectorizer(max_features=5000). You can adjust this parameter based on your specific needs and computational resources.\n",
    "\n",
    "We apply the Count Vectorizer to the 'v2' column of the DataFrame, transforming the text data into a numerical format suitable for machine learning models.\n",
    "\n",
    "If needed, we convert the result to a dense array using features = features.toarray(). This step may be necessary depending on the specific requirements of the downstream modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc50b148",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T10:21:02.283693Z",
     "iopub.status.busy": "2023-10-15T10:21:02.283311Z",
     "iopub.status.idle": "2023-10-15T10:21:02.505790Z",
     "shell.execute_reply": "2023-10-15T10:21:02.504486Z"
    },
    "id": "AisWyKW0TqyN",
    "papermill": {
     "duration": 0.233376,
     "end_time": "2023-10-15T10:21:02.508469",
     "exception": false,
     "start_time": "2023-10-15T10:21:02.275093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Convert tokenized words back to text\n",
    "data['v2'] = data['v2'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Initialize the Count Vectorizer\n",
    "count_vectorizer = CountVectorizer(max_features=5000)  # You can adjust max_features as needed\n",
    "\n",
    "# Apply the vectorizer to the 'v2' column\n",
    "features = count_vectorizer.fit_transform(data['v2'])\n",
    "\n",
    "# Convert the result to a dense array (if needed)\n",
    "features = features.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001aeadc",
   "metadata": {
    "id": "uMKzMNgYWKw8",
    "papermill": {
     "duration": 0.008046,
     "end_time": "2023-10-15T10:21:02.524153",
     "exception": false,
     "start_time": "2023-10-15T10:21:02.516107",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#Train-Test Split:\n",
    "\n",
    "Split your data into training and testing sets. This allows you to evaluate the performance of your model on data it hasn't seen before.\n",
    "\n",
    "#Explanation:\n",
    "\n",
    "We import the train_test_split function from scikit-learn, which allows us to split the dataset into training and testing sets.\n",
    "\n",
    "We use train_test_split to split the features (features) and labels (data['v1']) into training and testing sets. The parameter test_size=0.2 indicates that 20% of the data will be used for testing, while 80% will be used for training.\n",
    "\n",
    "The random_state=42 ensures that the split is reproducible. The same random state will produce the same split each time the code is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "352c7b4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T10:21:02.541747Z",
     "iopub.status.busy": "2023-10-15T10:21:02.541106Z",
     "iopub.status.idle": "2023-10-15T10:21:02.673905Z",
     "shell.execute_reply": "2023-10-15T10:21:02.672385Z"
    },
    "id": "7krM4O0ISwPv",
    "papermill": {
     "duration": 0.14505,
     "end_time": "2023-10-15T10:21:02.676887",
     "exception": false,
     "start_time": "2023-10-15T10:21:02.531837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, data['v1'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48df5a63",
   "metadata": {
    "id": "9tYU6cHxWNfz",
    "papermill": {
     "duration": 0.00686,
     "end_time": "2023-10-15T10:21:02.690926",
     "exception": false,
     "start_time": "2023-10-15T10:21:02.684066",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#Model Selection:\n",
    "\n",
    "Chose the Multinomial Naive Bayes classifier for its effectiveness in text classification tasks.\n",
    "\n",
    "#Explanation:\n",
    "\n",
    "We import the MultinomialNB class from scikit-learn, which represents the Multinomial Naive Bayes classifier.\n",
    "\n",
    "We initialize an instance of the Multinomial Naive Bayes classifier as clf. This classifier is a commonly used algorithm for text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a058cbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T10:21:02.710074Z",
     "iopub.status.busy": "2023-10-15T10:21:02.709089Z",
     "iopub.status.idle": "2023-10-15T10:21:02.725154Z",
     "shell.execute_reply": "2023-10-15T10:21:02.724286Z"
    },
    "id": "rlmDEufFTzqF",
    "papermill": {
     "duration": 0.029311,
     "end_time": "2023-10-15T10:21:02.727938",
     "exception": false,
     "start_time": "2023-10-15T10:21:02.698627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d644847",
   "metadata": {
    "id": "SCub0cC3WQ2z",
    "papermill": {
     "duration": 0.007494,
     "end_time": "2023-10-15T10:21:02.746253",
     "exception": false,
     "start_time": "2023-10-15T10:21:02.738759",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#Model Training:\n",
    "\n",
    "Train your chosen model on the training data.\n",
    "\n",
    "#Explanation:\n",
    "\n",
    "We use the fit method of the classifier (clf) to train it on the training data. The training data consists of the features (X_train) and their corresponding labels (y_train). This step allows the classifier to learn patterns in the data and make predictions on new, unseen examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e97f8fff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T10:21:02.761589Z",
     "iopub.status.busy": "2023-10-15T10:21:02.761103Z",
     "iopub.status.idle": "2023-10-15T10:21:02.913538Z",
     "shell.execute_reply": "2023-10-15T10:21:02.912292Z"
    },
    "id": "6r_zCUWiT1GA",
    "outputId": "8645bd23-8468-4b46-85ec-edb4e293e23b",
    "papermill": {
     "duration": 0.163022,
     "end_time": "2023-10-15T10:21:02.916096",
     "exception": false,
     "start_time": "2023-10-15T10:21:02.753074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755ab5bb",
   "metadata": {
    "id": "KMnyLqmdWULs",
    "papermill": {
     "duration": 0.007776,
     "end_time": "2023-10-15T10:21:02.931145",
     "exception": false,
     "start_time": "2023-10-15T10:21:02.923369",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#Model Evaluation:\n",
    "\n",
    "Evaluate the model's performance using metrics like accuracy, precision, recall, and F1-score.\n",
    "\n",
    "we evaluate the performance of the Multinomial Naive Bayes classifier using various classification metrics.\n",
    "\n",
    "#Explanation:\n",
    "\n",
    "We import the classification_report function from scikit-learn, which generates a detailed classification report.\n",
    "\n",
    "We use the trained classifier (clf) to make predictions on the test data (X_test).\n",
    "\n",
    "The classification_report function takes the true labels (y_test) and the predicted labels (y_pred) as input, and calculates various metrics including precision, recall, F1-score, and support for each class.\n",
    "\n",
    "The resulting report is printed, providing a comprehensive assessment of the classifier's performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "415f9686",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T10:21:02.949306Z",
     "iopub.status.busy": "2023-10-15T10:21:02.948078Z",
     "iopub.status.idle": "2023-10-15T10:21:03.007125Z",
     "shell.execute_reply": "2023-10-15T10:21:03.005394Z"
    },
    "id": "wvkvCho9T2kP",
    "outputId": "19bd37a2-2dc0-4e1a-a2fa-9ce5555aa683",
    "papermill": {
     "duration": 0.071724,
     "end_time": "2023-10-15T10:21:03.011308",
     "exception": false,
     "start_time": "2023-10-15T10:21:02.939584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       889\n",
      "           1       0.90      0.94      0.92       145\n",
      "\n",
      "    accuracy                           0.98      1034\n",
      "   macro avg       0.94      0.96      0.95      1034\n",
      "weighted avg       0.98      0.98      0.98      1034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0d6247",
   "metadata": {
    "id": "MmnqXzI-hvZw",
    "papermill": {
     "duration": 0.020009,
     "end_time": "2023-10-15T10:21:03.051922",
     "exception": false,
     "start_time": "2023-10-15T10:21:03.031913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "we perform k-fold cross-validation to assess the performance of the Multinomial Naive Bayes classifier.\n",
    "\n",
    "#Explanation:\n",
    "\n",
    "We import the cross_val_score function from scikit-learn, which performs k-fold cross-validation.\n",
    "\n",
    "We initialize a new instance of the Multinomial Naive Bayes classifier (clf) for cross-validation.\n",
    "\n",
    "Assuming 'features' (X) and labels (y) are available, we use cross_val_score to perform 5-fold cross-validation (cv=5). You can adjust the value of cv based on your specific requirements.\n",
    "\n",
    "The cross-validation scores are printed, showing the performance of the classifier in each fold, as well as the mean cross-validation score. This provides a more robust evaluation of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fb8cc2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T10:21:03.095575Z",
     "iopub.status.busy": "2023-10-15T10:21:03.094801Z",
     "iopub.status.idle": "2023-10-15T10:21:04.356307Z",
     "shell.execute_reply": "2023-10-15T10:21:04.354536Z"
    },
    "id": "bzxbJkX5VDLg",
    "outputId": "b3e7f5f1-a423-45ba-d2dc-4ee448c18680",
    "papermill": {
     "duration": 1.288163,
     "end_time": "2023-10-15T10:21:04.360396",
     "exception": false,
     "start_time": "2023-10-15T10:21:03.072233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.97969052 0.97582205 0.97582205 0.9787234  0.9767667 ]\n",
      "Mean CV Score: 0.9773649452028887\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Assuming 'features' and 'data['v1']' are your features and labels\n",
    "X = features\n",
    "y = data['v1']\n",
    "\n",
    "# Initialize a Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Perform 5-fold cross-validation (you can adjust 'cv' as needed)\n",
    "cv_scores = cross_val_score(clf, X, y, cv=5)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(f'Cross-Validation Scores: {cv_scores}')\n",
    "print(f'Mean CV Score: {cv_scores.mean()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.258926,
   "end_time": "2023-10-15T10:21:05.197668",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-15T10:20:53.938742",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
